## Folder Structure:
# ai-doc-chatbot/
# â”œâ”€â”€ app/streamlit_app.py
# â”œâ”€â”€ src/loader.py
# â”œâ”€â”€ src/vector_store.py
# â”œâ”€â”€ src/chatbot.py
# â”œâ”€â”€ src/local_llm.py
# â”œâ”€â”€ models/llama-2-7b-chat.Q4_K_M.gguf  # Downloaded model
# â”œâ”€â”€ requirements.txt
# â”œâ”€â”€ .gitignore
# â””â”€â”€ README.md

# --------------------------------------
# src/loader.py
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

def load_and_split(file_path):
    loader = PyPDFLoader(file_path)
    documents = loader.load()
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)
    return splitter.split_documents(documents)

# --------------------------------------
# src/vector_store.py
from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings

def create_vector_store(docs):
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    db = FAISS.from_documents(docs, embeddings)
    return db

# --------------------------------------
# src/local_llm.py
from llama_cpp import Llama

llm = Llama(model_path="models/llama-2-7b-chat.Q4_K_M.gguf", n_ctx=2048, n_threads=4)

def run_llama(prompt):
    full_prompt = f"[INST] {prompt} [/INST]"
    output = llm(full_prompt, max_tokens=300, temperature=0.7)
    return output['choices'][0]['text'].strip()

# --------------------------------------
# src/chatbot.py
from src.local_llm import run_llama

def create_qa_chain(db):
    retriever = db.as_retriever()

    def custom_rag(query):
        docs = retriever.get_relevant_documents(query)
        context = "\n\n".join(doc.page_content for doc in docs[:3])
        prompt = f"""Answer the question using the context below:

        CONTEXT:
        {context}

        QUESTION:
        {query}
        """
        return run_llama(prompt)

    return custom_rag

# --------------------------------------
# app/streamlit_app.py
import streamlit as st
from src.loader import load_and_split
from src.vector_store import create_vector_store
from src.chatbot import create_qa_chain

st.set_page_config(page_title="ðŸ“„ Doc Chatbot (Local)", page_icon="ðŸ“„")
st.title("ðŸ“„ Chat with Your PDF Document (Fully Local)")

uploaded_file = st.file_uploader("Upload a PDF", type=["pdf"])

if uploaded_file:
    with open(f"data/{uploaded_file.name}", "wb") as f:
        f.write(uploaded_file.read())

    with st.spinner("Processing document..."):
        docs = load_and_split(f"data/{uploaded_file.name}")
        db = create_vector_store(docs)
        rag = create_qa_chain(db)
        st.success("Chatbot is ready!")

    query = st.text_input("Ask a question from the document:")
    if query:
        with st.spinner("Generating answer..."):
            result = rag(query)
            st.write(result)

# --------------------------------------
# requirements.txt
# streamlit
# langchain
# faiss-cpu
# sentence-transformers
# llama-cpp-python
# python-dotenv
# PyPDF2
# tiktoken

# --------------------------------------
# .gitignore
__pycache__/
.env
models/*
data/*
*.pdf

# --------------------------------------
# README.md (summary)
# AI Chatbot for Documents (Fully Local)
# - Uses MiniLM + FAISS + LLaMA-2 7B (GGUF) with llama-cpp-python
# - Streamlit frontend to upload and chat with PDF documents
# - 100% offline, no API keys required
